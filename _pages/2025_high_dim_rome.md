# Mathematical methods for high-dimensional data
Summer school at Sapienza University of Rome, September 8-12, 2025



## Short course on Dynamics and learning in recurrent neural networks

### Lecture 1: Equilibrium theory of binary attractor networks
Based on:

* Amit, [Modeling brain function: The world of attractor neural networks](https://www.cambridge.org/core/books/modeling-brain-function/2EA95FDABF616D187220A6B9596091B7) (1989)
* Coolen, [Statistical mechanics of recurrent neural networks I — Statics](https://www.sciencedirect.com/science/article/abs/pii/S1383812101800178) (2001)

_Recommended readings:_
* Mézard, Parisi, Virasoro, [Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications](https://www.worldscientific.com/worldscibooks/10.1142/0271?srsltid=AfmBOorX3r0G7--Yh3FYwVXlrmy7dkjANP4ocfLfW9kVeSXtLgsMa-h3#t=aboutBook) (1986)
* Castellani, Cavagna, [Spin-glass theory for pedestrians](https://iopscience.iop.org/article/10.1088/1742-5468/2005/05/P05012) (2005)

### Lecture 2: Dynamics of binary attractor networks

Based on:
* Coolen, [Statistical mechanics of recurrent neural networks II — Dynamics](https://arxiv.org/abs/cond-mat/0006011) (2000)

_Recommended readings:_
* Fischer, Hertz, [Spin Glasses](https://www.cambridge.org/core/books/spin-glasses/8513DA3DC0EE8370FF6E0AC5248825DF) (1993)
* Eissfeller, Opper [Mean-field Monte Carlo approach to the Sherrington-Kirkpatrick model with asymmetric couplings](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.50.709) (1994)

### Lecture 3: Chaotic dynamics in random neural networks
Based on:
* Sompolinsky, Crisanti, Sommers, [Chaos in Random Neural Networks](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.61.259) (1988)
* Rajan, Abbott, Sompolinsky, [Stimulus-dependent suppression of chaos in recurrent neural networks](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.82.011903) (2010)
* Helias, Dahmen, [Statistical Field Theory for Neural Networks](https://link.springer.com/book/10.1007/978-3-030-46444-8) (2020)

_Recommended readings:_
* Hertz, Roudi, Sollich, [Path integral methods for the dynamics of stochastic and disordered systems](https://iopscience.iop.org/article/10.1088/1751-8121/50/3/033001/meta?casa_token=F-MfH5MFzVQAAAAA:vSpHyBSfPHWX2nUOwh_ms5I18nKW1F7f0aVdl-zeJq7mqC1L9or03GV80kG489uHyulerg-ArMSs6GyriQWPSu9x2xxkrw) (2016)
* Roy, Biroli, Bunin, Cammarota, [Numerical implementation of dynamical mean field theory for disordered systems: application to the Lotka–Volterra model of ecosystems](https://iopscience.iop.org/article/10.1088/1751-8121/ab1f32/meta?casa_token=7OZ5U5ypln4AAAAA:E7nYkRRK8r_1S2zlXTNkul5o0NRJfcPeYfFbzQl0YbsX0RdiTLvvY7g1RYH54KndLcDzeWR2XqUioUtMWfrC9MSZPx8WwQ) (2019)
* Clark, Abbott, Litwin-Kumar, [Dimension of Activity in Random Neural Networks](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.118401) (2023)
* Galla, [Generating-functional analysis of random Lotka-Volterra systems: A step-by-step guide](https://arxiv.org/abs/2405.14289) (2024)
* Clark, Abbott, [Theory of Coupled Neuronal-Synaptic Dynamics](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.14.021001) (2024)
* Clark, Marschall, van Meegen, Litwin-Kumar, [Connectivity structure and dynamics of nonlinear recurrent neural networks](https://arxiv.org/abs/2409.01969) (2025)

### Lecture 4: Training recurrent networks
Based on:
* Jager, [Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach](https://mineracaodedados.wordpress.com/wp-content/uploads/2015/01/jaeger_trainingrnnstutorial-2005.pdf) (2002)
* Sussillo, Abbott, [Generating Coherent Patterns of Activity from Chaotic Neural Networks](https://www.cell.com/AJHG/fulltext/S0896-6273(09)00547-9) (2009)
* Goodfellow, Bengio, Courville, [Deep learning](https://www.deeplearningbook.org/) (2016)

_Recommended readings:_
* Sussillo, Barak, [Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks](https://direct.mit.edu/neco/article-abstract/25/3/626/7854/Opening-the-Black-Box-Low-Dimensional-Dynamics-in) (2013)
* Couillet, Wainrib, Tiomoko Ali, Sevi, [A Random Matrix Approach to Echo-State Neural Networks](https://proceedings.mlr.press/v48/couillet16.html) (2016)
* Rivkind, Barak, [Local Dynamics in Trained Recurrent Neural Networks](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.118.258101) (2017)
* Mastrogiuseppe, Ostojic, [Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks](https://www.cell.com/neuron/fulltext/S0896-6273(18)30543-9) (2018)
* Schuessler, Mastrogiuseppe, Dubreuil, Ostojic, Omri Barak, [The interplay between randomness and structure during learning in RNNs](https://proceedings.neurips.cc/paper/2020/hash/9ac1382fd8fc4b631594aa135d16ad75-Abstract.html) (2020)
* Fanthomme, Monasson, [Low-Dimensional Manifolds Support Multiplexed Integrations in Recurrent Neural Networks](https://direct.mit.edu/neco/article-abstract/33/4/1063/97474/Low-Dimensional-Manifolds-Support-Multiplexed) (2021)
* Fournier, Urbani, [Statistical physics of learning in high-dimensional chaotic systems](https://iopscience.iop.org/article/10.1088/1742-5468/ad082d/meta) (2023)
* Bordelon, JordanCotler, Pehlevan, Zavatone-Veth, [Dynamically Learning to Integrate in Recurrent Neural Networks](https://arxiv.org/abs/2503.18754) (2025)
* Hakim, Karma, [Theory of Temporal Pattern Learning in Echo State Networks](https://www.biorxiv.org/content/10.1101/2025.06.23.661158v1.abstract) (2025)